---
title: "Mixed effect models in association studies"
author: "Xulong Wang"
output: pdf_document
---

# the problem

Population structure and relatedness, as a confounding factor, generates false positive findings, which is the serious concern in association studies.
Confounding variable is a variable that correlates with both the dependent and independent variables.

Why population structure is a confounding factor? There are multipe ways:

> Case 1

1. Population subgroups often share different dietary habits or life style characteristics (non-genetic factors) that leads to many traits of interest being different among the groups (correlation);
2. Many genetic markers that are not influencing the disease (not causal) have significant differences in allele frequencies among the population (another correlation); 
3. False positive or spurious association appears

```{r}
set.seed(1)
y.p1 = rnorm(1e3, 3, 3)
y.p2 = rnorm(1e3, 7, 3)

x.p1 = rbinom(1e3, 1, 0.3)
x.p2 = rbinom(1e3, 1, 0.6)

summary(lm(y.p1 ~ x.p1))
summary(lm(y.p2 ~ x.p2))

pcts = seq(0, 1, 1e-3)
pval = sapply(pcts, function(pct) {
  y.all = c(sample(y.p1, pct * 1e3), sample(y.p2, (1-pct) * 1e3))
  x.all = c(sample(x.p1, pct * 1e3), sample(x.p2, (1-pct) * 1e3))
  fit = summary(lm(y.all ~ x.all))
  fit$coefficients["x.all", "Pr(>|t|)"]
})
plot(pcts, pval, type = "b")
```

> Case 2

1. Phenotypes are not different among the subpopulations;
2. Study objects are not randomly choosen (complete randomness is hard), so that sample of cases are overrepresented in one subpopulation, and maybe underrepresented in another subpopulation;
3. This is essentially making pseudo-subpopulations of different phenotypic traits; 
4. Genetic markers that are not influcencing the disease have significant differences in allele frequencies among the population (this is common); 
5. False positive or spurious association appears

> What happens if allele frequencies are not different among subpopulations, but phenotypes are?

Like case 2, this makes false positive findings, if certain genotype (certain mutation) are overrepresented in one subpopulation. 

As a summary, spurious correlation could appear as long as there are systematic differences of phenotype or genotype among the subpopulations. The mission is to break the spurious correlations by modeling the confounding factor: the subpopulation structure.

```{r}
pct = .5
y.all = c(sample(y.p1, pct * 1e3), sample(y.p2, (1-pct) * 1e3))
x.all = c(sample(x.p1, pct * 1e3), sample(x.p2, (1-pct) * 1e3))
index = c(rep(0, pct * 1e3), rep(1, (1-pct) * 1e3))
summary(lm(y.all ~ x.all + index))

pcts = seq(0, 1, 1e-3)
pval = sapply(pcts, function(pct) {
  y.all = c(sample(y.p1, pct * 1e3), sample(y.p2, (1-pct) * 1e3))
  x.all = c(sample(x.p1, pct * 1e3), sample(x.p2, (1-pct) * 1e3))
  index = c(rep(0, pct * 1e3), rep(1, (1-pct) * 1e3))
  fit = summary(lm(y.all ~ x.all + index))
  fit$coefficients["x.all", "Pr(>|t|)"]
})
plot(pcts, pval, type = "b")
```

The exact population structures are often unknown in reality. To resolve this problem leads to mixed effect modeling or principal component analysis methods. 

# pca

```{r}
svd()
```

# mixed effect model

# Model

$$ Y = X\beta + g + \epsilon$$
$$ g \sim N(0, \sigma_g^2 * K) $$
$$ \epsilon \sim N(0, \sigma_e^2* I) $$

1. $g$ is a length $n$ random vector of polygenic effects
2. $\sigma_g^2$ is additive genetic variance due to polygenic effects
3. $\sigma_e^2$ is non-genetic variance due to non-genetic effects

```{r}
g = c(2, 2, 1, 1, 0)
p = c(1, 1, 0, 0, 0)
cor.test(g, p)
```

```{r}
library(MASS)
library(Matrix)

# make a pseudo-kinship matrix
n = 5
set.seed(3)
k = matrix(rnorm(n*n, 3, 3), n, n)
k[k <= 0] = .1
k = k / max(k)
k[upper.tri(k)] = t(k)[upper.tri(k)]
diag(k) = 1
k = nearPD(k)$mat

# y that follows the variance-covariance structure 
y <- mvrnorm(n = 1e3, mu = rep(0, n), Sigma = k)
colMeans(y)
var(y)
var(y[, 1])
var(y[, 1], y[, 2])

# the most correlated pair 
k2 = var(y)
diag(k2) = 0
which(k2 == max(k2), arr.ind = T)
cor(y[, 2], y[, 5])

# y values of the most correlated pair are more similar
y[1, ]
y[2, ]
y[3, ]
```

# pca?

Example in Price et al., 2006 Nature Genetics.
> Principle components analysis corrects for stratification in genome-wide association studies

```{r}
g = matrix(c(1, 1, 1, 0, 0,
             0, 1, 2, 1, 2,
             2, 1, 1, 0, 1,
             0, 0, 1, 2, 2,
             2, 1, 1, 0, 0,
             0, 0, 1, 1, 1,
             2, 2, 1, 1, 0), byrow = T, nrow = 7, ncol = 5)

pca = prcomp(g, scale = T)
pca$rotation

p = c(1, 1, 0, 0, 0)
summary(lm(p ~ g[7, ]))
summary(lm(p ~ pca$rotation[, 1] + g[7, ]))

svd = svd(g)
svd$v

plot(svd$v[, 2])
summary(lm(p ~ svd$v[, 2] + g[7, ]))
```

I can see how LMM and PCA work in caputuring the polygenic effects, and adjust for it when estimating effect of a specific variant.

How PCA & LMM methods perform differently in more complicated situations, with large sample size and cryptic relatedness?
> It would be nice to do real analysis using simulation or real datasets.

Literatures say PCA can not handle relatedness, but why?
> I guess PCA just failed to caputure that information, when that information is not that much, therefore flooded in the total genotypic matrix.

Further, LMM is theoretically appealing, what are the pitfalls?
> Alkes Price passed on a few highly relevant papers.

# the end as of 3/2/2018

--------------------------------------------

```{r}
load("adsp.rdt")
mdata <- adsp$mdata[1:100, ]
Sigma <- adsp$kinship$autosome[1:100, 1:100]
Sigma[Sigma < 0] <- 0

model <- stan_model("mvn.stan") 

dat1 <- list(L = nrow(mdata), y = mdata$AD1, Sigma = Sigma, prior = 0) 
dat2 <- list(L = nrow(mdata), y = mdata$AD1, Sigma = Sigma, prior = 2) 
dat3 <- list(L = nrow(mdata), y = mdata$AD1, Sigma = Sigma, prior = 4) 

opt <- optimizing(model, data = dat1)
opt$par[c("mu", "sigma", "epsilon", "z[1]", "u[1]")]

opt <- optimizing(model, data = dat2)
opt$par[c("mu", "sigma", "epsilon", "z[1]", "u[1]")]

opt <- optimizing(model, data = dat3)
opt$par[c("mu", "sigma", "epsilon", "z[1]", "u[1]")]

```

1. Random effect was tiny
2. Prior takes effect in optimizing()
3. As long as prior wasn't extremely ill, it does not affect the inference in noticable scale

```{r}
fit <- sampling(model, chain = 2, data = dat1, iter = 600, warmup = 200)
print(fit, pars = c("mu", "sigma", "epsilon", "z[1]", "u[1]"))

fit <- sampling(model, chain = 2, data = dat2, iter = 600, warmup = 200)
print(fit, pars = c("mu", "sigma", "epsilon", "z[1]", "u[1]"))
```

1. Random effect was also small, but much larger than the optimizing() result
2. $\sigma$ clearly higher and comparable with $\epsilon$ in sampling()

# Run sampling() on selected markers

```{r}
sample = extract(fit)

C2 <- sample$C2
C3 <- chol(Sigma)

t(C3)[1:5, 1:5] 
C2[1, 1:5, 1:5]
```

$$C_{stan} = (chol(K))^T$$

```{r}
u = sample$u
u2 = sample$u2

all(u == u2)

z = sample$z
my_u <- sample$sigma[1] * t(chol(Sigma)) %*% z[1, ]

u[1, 1:10]
my_u[1:10, ]
```

Stan use same mechanism for "generated quantities" and "transformed parameters": direct computing, no estimation

```{r}
cov = cov(u)

cor(Sigma[1, ], cov(u)[1, ])
cor(Sigma[2, ], cov(u)[2, ])
cor(Sigma[3, ], cov(u)[3, ])
```

1. Covariances of the random samples are not Sigma. 
2. We permit the random effect to be estimated again for each model
3. We let covariates and response information flowing back to help estimating the random effect

# Models

```{r}
stan_multi_norm <- "

data {
int<lower=1> K; # outcomes
vector[K] mu;
cov_matrix[K] Sigma;
vector[K] y;
}

parameters {
vector[K] beta; 
}

model {
beta ~ multi_normal(mu, Sigma);
}

"

stan_multi_cholesky <- "

data {
int<lower=1> K; # outcomes
vector[K] mu;
cov_matrix[K] Sigma;
vector[K] y;
}

transformed data {
matrix[K, K] L;
L <- cholesky_decompose(Sigma);
}

parameters {
vector[K] beta; 
}
model {
beta ~ multi_normal_cholesky(mu, L);
}
"

load("adsp.rdt")
mdata <- adsp$mdata
Sigma <- adsp$kinship$autosome
Sigma[Sigma < 0] <- 0

stan_multi_norm <- stan_model(model_code = stan_multi_norm)
stan_multi_cholesky <- stan_model(model_code = stan_multi_cholesky) 

dat <- list(K = 576, mu = rep(0, 576), y = mdata$AD1, Sigma = Sigma) 

fit_multi_norm <- sampling(stan_multi_norm, chain = 2, data = dat) # 450 sec each chain
fit_multi_cholesky <- sampling(stan_multi_cholesky, data = dat) # 170 sec each chain 
```
